import logging
import os
import re
from typing import List, Optional, Set

import nltk
import numpy as np
from numpy.linalg import norm
from thefuzz import fuzz

from datahub_classify.helper_classes import TextEmbeddings

logger = logging.getLogger(__name__)
GLOVE_URL = "https://nlp.stanford.edu/data/glove.6B.zip"


def load_stopwords() -> Set[str]:
    """Load Stopwords"""
    try:
        from nltk.corpus import stopwords

        stop_words = set(stopwords.words("english"))

    except Exception as e:
        logger.warning(
            f"Could not Load Stopwords due to {e}: Downloading Stopwords......."
        )
        nltk.download("stopwords")
        from nltk.corpus import stopwords

        stop_words = set(stopwords.words("english"))
    return stop_words


def cosine_similarity_score(vec1: np.ndarray, vec2: np.ndarray) -> Optional[float]:
    """Cosine similarity measures the similarity between two vectors of an inner product space.
    This function is used to calculate the cosine similarity between vector representations of two strings
    """
    try:
        cos_sim = np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))
    except ValueError as e:
        logger.exception(f"Failed to get cosine similarity - \n {str(e)}")
        cos_sim = None
    if cos_sim is not None and cos_sim <= 0:
        cos_sim = 0
    return cos_sim


def read_glove_vector(glove_vector: str) -> dict:
    """
    This function reads an existing word to vector mappings from the file and returns a dict where words are keys and their glove vectors are values

    Glove vector: vector representation of a word generated by an unsupervised learning algorithm 'GloVe' which captures the semantic meaning of a words to some extent assigns nearby vectors to words similar in meaning
    """
    with open(glove_vector, "r", encoding="UTF-8") as f:
        word_to_vec_map = {}
        for line in f:
            w_line = line.split()
            curr_word = w_line[0]
            word_to_vec_map[curr_word] = np.array(w_line[1:], dtype=np.float64)
    return word_to_vec_map


def get_fuzzy_score(
    text_1: str,
    text_2: str,
    text_type: str,
    text_1_words: List[str] = [],
    text_2_words: List[str] = [],
) -> Optional[float]:
    try:
        fuzzy_match_score = fuzz.token_set_ratio(text_1, text_2) / 100
        if fuzzy_match_score <= 0.5:
            fuzzy_match_score = 0.8 * fuzzy_match_score
        max_fuzz_score = 0
        if text_type == "name":
            if len(text_1_words) == 1 or len(text_2_words) == 1:
                for word_1 in text_1_words:
                    for word_2 in text_2_words:
                        fuzz_score = fuzz.token_set_ratio(word_1, word_2) / 100
                        if fuzz_score > max_fuzz_score:
                            max_fuzz_score = fuzz_score
        fuzzy_match_score = np.maximum(fuzzy_match_score, max_fuzz_score)
    except Exception as e:
        logger.exception(f"Fuzzy Score Not calculated {e}")
        fuzzy_match_score = None
    return fuzzy_match_score


def get_embedding_score(
    text_1_emb: List[TextEmbeddings],
    text_2_emb: List[TextEmbeddings],
    text_1_words: List[str],
    text_2_words: List[str],
    word_to_vec_map: dict,
) -> Optional[float]:
    emb_1 = None
    emb_2 = None
    try:
        if len(text_1_words) == 1 and len(text_2_words) == 1:
            glove_emb_1 = word_to_vec_map.get(text_1_words[0], None)
            glove_emb_2 = word_to_vec_map.get(text_2_words[0], None)

            if glove_emb_1 is not None and glove_emb_2 is not None:
                emb_1 = glove_emb_1
                emb_2 = glove_emb_2
            else:
                for text_emb in text_1_emb:
                    if text_emb.emb_type == "sentence_transformer":
                        emb_1 = text_emb.embedding
                        break
                for text_emb in text_2_emb:
                    if text_emb.emb_type == "sentence_transformer":
                        emb_2 = text_emb.embedding
                        break
        else:
            for text_emb in text_1_emb:
                if text_emb.emb_type == "sentence_transformer":
                    emb_1 = text_emb.embedding
                    break
            for text_emb in text_2_emb:
                if text_emb.emb_type == "sentence_transformer":
                    emb_2 = text_emb.embedding
                    break
        if emb_1 is None or emb_2 is None:
            raise Exception("Sentence Transformer Embeddings Not Found!!!")
        emb_match_score = cosine_similarity_score(emb_1, emb_2)
        return emb_match_score
    except Exception as e:
        logger.exception(f"Failed to calculate cosine similarity {e}")
    return None


def compute_string_similarity(
    text_1: Optional[str],
    text_2: Optional[str],
    text_1_emb: List[TextEmbeddings],
    text_2_emb: List[TextEmbeddings],
    text_type: str,
    word_to_vec_map: dict,
    stop_words: set,
    use_embeddings: bool,
) -> Optional[float]:
    try:
        if (
            text_1 is not None
            and text_1.strip() != ""
            and text_2 is not None
            and text_2.strip() != ""
        ):
            # Text pre Processing
            text_1 = text_1.lower().strip()
            text_2 = text_2.lower().strip()
            text_1_cleaned = re.sub(r"[^a-z]", " ", text_1.lower()).strip()
            text_2_cleaned = re.sub(r"[^a-z]", " ", text_2.lower()).strip()
            text_1_words = [
                word for word in text_1_cleaned.split() if word not in stop_words
            ]
            text_2_words = [
                word for word in text_2_cleaned.split() if word not in stop_words
            ]
            # Calculate Embedding Score
            if use_embeddings:
                if len(text_1_emb) > 0 and len(text_2_emb) > 0:
                    emb_match_score = get_embedding_score(
                        text_1_emb=text_1_emb,
                        text_2_emb=text_2_emb,
                        text_1_words=text_1_words,
                        text_2_words=text_2_words,
                        word_to_vec_map=word_to_vec_map,
                    )
                else:
                    raise Exception(
                        "Embeddings must be provided when 'use_embeddings = True'"
                    )
            else:
                emb_match_score = 0.0
            # Calculate fuzzy score
            fuzzy_match_score = get_fuzzy_score(
                text_1=text_1,
                text_2=text_2,
                text_type=text_type,
                text_1_words=text_1_words,
                text_2_words=text_2_words,
            )

            if emb_match_score is None and fuzzy_match_score is None:
                score = None
            else:
                if emb_match_score is None:
                    emb_match_score = 0.0
                if fuzzy_match_score is None:
                    fuzzy_match_score = 0.0
                score = np.maximum(fuzzy_match_score, emb_match_score)
        else:
            score = None
    except Exception as e:
        logger.exception(
            f"Failed to find name / description similarity for texts: '{text_1}' and '{text_2}' due to {e}",
        )
        score = None
    return score


def download_glove_embeddings(glove_vec):
    try:
        destination_path, glove_file = os.path.split(glove_vec)
        from io import BytesIO
        from zipfile import ZipFile

        import requests

        # URL = "https://nlp.stanford.edu/data/glove.6B.zip"
        response = requests.get(GLOVE_URL)
        zip_object = ZipFile(BytesIO(response.content))
        zip_object.extract(glove_file, path=destination_path)
        logger.debug("Successfully Downloaded GLOVE Embeddings!!")
    except Exception as e:
        logger.exception(f"Unable To Download GLOVE Embeddings due to {e}")
